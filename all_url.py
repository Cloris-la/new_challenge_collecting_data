from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from tqdm import tqdm
import csv

# function tranfers to soup and extracts each page's url, return a list of all links
def extract_links(page):
    html = page.content()
    soup = BeautifulSoup(html, "html.parser")
    container = soup.find("div", class_="property-results_container")
    search_results = container.find_all("div", class_="property-item") if container else []
    links = []
    for search_result in search_results:
        link = search_result.find("a", class_="property-item_link")
        if link and "href" in link.attrs:
            links.append("https://www.zimmo.be" + link["href"])
    return links

all_links = []
# main program for request pages which maintain 21 properties / page
with sync_playwright() as p:
    browser = p.chromium.launch(headless=False) #launch browser first
    page = browser.new_page()

    page.wait_for_timeout(2000) #timeout 2 seconds

    for i in tqdm(range(1, 3)):  # loop 2 pages
        url = f'https://www.zimmo.be/fr/rechercher/?search=eyJmaWx0ZXIiOnsic3RhdHVzIjp7ImluIjpbIkZPUl9TQUxFIiwiVEFLRV9PVkVSIl19LCJ6aW1tb0NvZGUiOnsibm90SW4iOlsiTDgwTjUiLCJLTUQ4MCIsIkw0RVhHIiwiTDBBUjIiXX19LCJzb3J0aW5nIjpbeyJ0eXBlIjoiUkFOS0lOR19TQ09SRSIsIm9yZGVyIjoiREVTQyJ9XSwicGFnaW5nIjp7ImZyb20iOjE3LCJzaXplIjoyMX19&p={i}#gallery'    
        page.goto(url)
        page.wait_for_timeout(2000)
        link = extract_links(page)
        all_links.extend(link)

    browser.close()

# using set to automatically remove same link and tranfer to a list
all_links = list(set(all_links))
# saving all links into a txt file
with open("zimmoweb_links.txt", "w", encoding="utf-8") as f:
    for link in all_links:
        f.write(link + "\n")

print(f"ğŸ‰ å…±ä¿å­˜äº† {len(all_links)} ä¸ªæˆ¿äº§é“¾æ¥")


# =============== extract_property_info=======================

# get features under ul.main_features
def extract_main_features (soup) :
    data = {'Type':"N/A",
            "Surf.habitable":"N/A",
            "Chambre":"N/A",
            'Salles de bain':"N/A",
            'Construit en':"N/A",
            'PEB':'N/A',
            'Obligation de rÃ©novation':"N/A",
            'RC':"N/A"}
    features = {}
    feature_list = soup.find('ul',class_='main-features')
    if feature_list :
        for li in feature_list :
            lable = li.find('strong')
            value = li.find('span')
            if lable and value :
                key = lable.get_text(strip = True)
                val = value.get_text(strip = True)
                if key == "Type" :
                    data['Type'] = val
                elif key == 'Surf.habitable':
                    data['Surf.habitable'] = val
                elif key == "Chambre" :
                    data['Chambre'] = val
                elif key == 'Salles de bain':
                    data['Salles de bain'] = val
                elif key == 'Construit en':
                    data['Construit en'] = val
                elif key == 'PEB':
                    data['PEB'] = val
                elif key == 'Obligation de rÃ©novation':
                    data['Obligation de rÃ©novation'] = val
                elif key == 'RC':
                    data['RC'] = val
    return data

def extract_property_info(page,url) :
    try:
        page.goto(url,timeout = 2000)
        page.wait_for_timeout(3000)
        soup = BeautifulSoup(page.content(),'html.parser')

        # CSS selector
        locality = soup.find('h2',class_='section-title __full-address')
        #price = soup.find('span',class_='feature-value ')
        features = extract_main_features(soup)
        return{
            "url":url,
            "type":features['Type'],
            "Surf.habitable":features['Surf.habitable'],
            "Chambre":features['Chambre'],
            'Salles de bain':features['Salles de bain'],
            'Construit en':features['Construit en'],
            'PEB':features['PEB'],
            'Obligation de rÃ©novation':features['Obligation de rÃ©novation'],
            'RC':features['RC']
        }

    except Exception as e:
        print(f"{e}")

## ===== è°ƒè¯• extract_property_info å‡½æ•° =====
print("\nğŸš€ æ­£åœ¨æµ‹è¯• extract_property_info ...")

# è¯»å–å‰å‡ æ¡é“¾æ¥è¿›è¡Œæµ‹è¯•
with open("zimmoweb_links.txt", "r", encoding="utf-8") as f:
    test_links = [line.strip() for line in f.readlines()[:3]]  # æµ‹è¯•å‰3æ¡é“¾æ¥

with sync_playwright() as p:
    browser = p.chromium.launch(headless=False)
    page = browser.new_page()

    for i, url in enumerate(test_links, 1):
        print(f"\nğŸ” æ­£åœ¨æå–ç¬¬ {i} æ¡ï¼š{url}")
        info = extract_property_info(page, url)
        if info:
            for k, v in info.items():
                print(f"{k}: {v}")

    browser.close()